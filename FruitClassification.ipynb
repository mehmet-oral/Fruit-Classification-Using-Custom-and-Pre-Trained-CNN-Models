{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "weBkDjgx4scj",
        "outputId": "fe80e7de-f2ee-4743-d682-a6bc6142ae71"
      },
      "outputs": [],
      "source": [
        "# Example usage:\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "nXtzXXLkzNXA",
        "outputId": "3a7a7f46-575e-4ffe-a350-063f7650292d"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "\n",
        "source_path = \"/content/drive/MyDrive/FRUIT_DATASET_24_CLASS\"\n",
        "destination_path = '/content/FRUIT_DATASET'\n",
        "\n",
        "shutil.copytree(source_path, destination_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GJzYlarm5mh2",
        "outputId": "0133fe5d-5eec-4f12-c259-8afccadf9234"
      },
      "outputs": [],
      "source": [
        "# 2. Define paths\n",
        "import numpy as np\n",
        "import pickle\n",
        "import os\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import random\n",
        "#import numpy as np\n",
        "\n",
        "\n",
        "base_dir = \"/content/FRUIT_DATASET\"\n",
        "\n",
        "train_path = os.path.join(base_dir, \"Training\")\n",
        "test_path = os.path.join(base_dir, \"Test\")\n",
        "\n",
        "\n",
        "def load_images_from_folder(folder_path, image_size=(100, 100)):\n",
        "    images = []\n",
        "    labels = []\n",
        "    class_names = sorted([d for d in os.listdir(folder_path) if not d.startswith('.') and os.path.isdir(os.path.join(folder_path, d))])\n",
        "    class_to_idx = {cls_name: idx for idx, cls_name in enumerate(class_names)}\n",
        "\n",
        "    for cls_name in class_names:\n",
        "        cls_folder = os.path.join(folder_path, cls_name)\n",
        "        if not os.path.isdir(cls_folder):\n",
        "            continue\n",
        "        for filename in os.listdir(cls_folder):\n",
        "            if filename.endswith(\".jpg\") and not filename.startswith('.'):\n",
        "                img_path = os.path.join(cls_folder, filename)\n",
        "                img = Image.open(img_path).convert(\"L\")  # convert to grayscale\n",
        "                img = img.resize(image_size)\n",
        "                img_array = np.asarray(img, dtype=np.float32) / 255.0       # normalize to [0, 1]\n",
        "                images.append(img_array)\n",
        "                labels.append(class_to_idx[cls_name])\n",
        "\n",
        "    return np.array(images), np.array(labels), class_names\n",
        "\n",
        "X_train, y_train, class_names = load_images_from_folder(train_path)\n",
        "X_test, y_test, _ = load_images_from_folder(test_path)\n",
        "\n",
        "\n",
        "print(\"Train set shape:\", X_train.shape)\n",
        "print(\"Train labels shape:\", y_train.shape)\n",
        "print(\"Test set shape:\", X_test.shape)\n",
        "print(\"Test labels shape:\", y_test.shape)\n",
        "print(\"Number of classes:\", len(class_names))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kus_bmqV6IIy",
        "outputId": "336a38c0-c2e3-4597-b663-3a77f26ba984"
      },
      "outputs": [],
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XzyB9OoI5IEY"
      },
      "outputs": [],
      "source": [
        "\n",
        "class DeepCNN:\n",
        "    def __init__(self, input_shape, num_classes, learning_rate=0.01):\n",
        "        self.input_shape = input_shape  # (height, width)\n",
        "        self.num_classes = num_classes\n",
        "        self.lr = learning_rate\n",
        "\n",
        "        # Conv layer 1: 8 filters\n",
        "        self.input_depth = 1  # For grayscale images\n",
        "        self.filters1 = np.random.randn(8, 3, 3, self.input_depth) / 9  # 8 filters of size 3x3\n",
        "\n",
        "        # Conv layer 2: 16 filters\n",
        "        self.filters2 = np.random.randn(16, 3, 3, 8) / 9  # 16 filters of size 3x3 with input depth 8\n",
        "\n",
        "        # Calculate the output sizes after each layer\n",
        "        h, w = input_shape\n",
        "\n",
        "        # Conv1 output size\n",
        "        self.conv1_out_h = h - 2  # valid padding\n",
        "        self.conv1_out_w = w - 2\n",
        "\n",
        "        # Pool1 output size\n",
        "        self.pool1_out_h = self.conv1_out_h // 2\n",
        "        self.pool1_out_w = self.conv1_out_w // 2\n",
        "\n",
        "        # Conv2 output size\n",
        "        self.conv2_out_h = self.pool1_out_h - 2  # valid padding\n",
        "        self.conv2_out_w = self.pool1_out_w - 2\n",
        "\n",
        "        # Pool2 output size\n",
        "        self.pool2_out_h = self.conv2_out_h // 2\n",
        "        self.pool2_out_w = self.conv2_out_w // 2\n",
        "\n",
        "        # Compute final flattened size after conv + pooling\n",
        "        self.flattened_size = 16 * self.pool2_out_h * self.pool2_out_w\n",
        "\n",
        "        # Fully connected layer\n",
        "        self.W = np.random.randn(self.num_classes, self.flattened_size) * 0.01\n",
        "        self.b = np.zeros((self.num_classes, 1))\n",
        "\n",
        "        print(f\"Network Architecture:\")\n",
        "        print(f\"Input: {input_shape}\")\n",
        "        print(f\"Conv1: {8}x{self.conv1_out_h}x{self.conv1_out_w}\")\n",
        "        print(f\"Pool1: {8}x{self.pool1_out_h}x{self.pool1_out_w}\")\n",
        "        print(f\"Conv2: {16}x{self.conv2_out_h}x{self.conv2_out_w}\")\n",
        "        print(f\"Pool2: {16}x{self.pool2_out_h}x{self.pool2_out_w}\")\n",
        "        print(f\"Flattened size: {self.flattened_size}\")\n",
        "        print(f\"Output: {self.num_classes}\")\n",
        "\n",
        "        # For storing intermediate values during forward pass (needed for backprop)\n",
        "        self.cache = {}\n",
        "\n",
        "    def conv_forward(self, x, filters, stride=1):\n",
        "        \"\"\"\n",
        "        Perform forward pass through convolutional layer.\n",
        "\n",
        "        Args:\n",
        "            x: Input of shape (H, W, C) or (H, W) for the first layer\n",
        "            filters: Filters of shape (F, Kh, Kw, C)\n",
        "            stride: Stride for convolution\n",
        "\n",
        "        Returns:\n",
        "            Output of shape (H', W', F)\n",
        "        \"\"\"\n",
        "        # Add channel dimension for first layer if needed\n",
        "        if x.ndim == 2:\n",
        "            x = x[..., np.newaxis]  # More efficient than reshape\n",
        "\n",
        "        h, w, c = x.shape\n",
        "        num_filters, kh, kw, _ = filters.shape\n",
        "\n",
        "        # Calculate output dimensions\n",
        "        out_h = (h - kh) // stride + 1\n",
        "        out_w = (w - kw) // stride + 1\n",
        "\n",
        "        # Initialize output\n",
        "        output = np.zeros((out_h, out_w, num_filters))\n",
        "\n",
        "        # Vectorized implementation\n",
        "        for f in range(num_filters):\n",
        "            for i in range(out_h):\n",
        "                for j in range(out_w):\n",
        "                    h_start = i * stride\n",
        "                    h_end = h_start + kh\n",
        "                    w_start = j * stride\n",
        "                    w_end = w_start + kw\n",
        "\n",
        "                    # Extract patch and compute convolution\n",
        "                    patch = x[h_start:h_end, w_start:w_end, :]\n",
        "                    output[i, j, f] = np.sum(patch * filters[f])\n",
        "\n",
        "        # Apply ReLU activation\n",
        "        return np.maximum(0, output)\n",
        "\n",
        "    def pool_forward(self, x, pool_size=2, stride=2):\n",
        "        \"\"\"\n",
        "        Perform max pooling.\n",
        "\n",
        "        Args:\n",
        "            x: Input of shape (H, W, C)\n",
        "            pool_size: Size of pooling window\n",
        "            stride: Stride for pooling\n",
        "\n",
        "        Returns:\n",
        "            Output of shape (H/stride, W/stride, C)\n",
        "        \"\"\"\n",
        "        h, w, c = x.shape\n",
        "\n",
        "        # Output dimensions\n",
        "        out_h = h // stride\n",
        "        out_w = w // stride\n",
        "\n",
        "        # Initialize output\n",
        "        output = np.zeros((out_h, out_w, c))\n",
        "\n",
        "        # Store pooling indices for backpropagation\n",
        "        pool_indices = np.zeros((out_h, out_w, c, 2), dtype=int)\n",
        "\n",
        "        # Perform max pooling\n",
        "        for i in range(out_h):\n",
        "            for j in range(out_w):\n",
        "                for k in range(c):\n",
        "                    h_start, w_start = i * stride, j * stride\n",
        "                    h_end, w_end = h_start + pool_size, w_start + pool_size\n",
        "\n",
        "                    # Extract the patch\n",
        "                    patch = x[h_start:h_end, w_start:w_end, k]\n",
        "\n",
        "                    # Find the max value and its indices\n",
        "                    max_idx = np.unravel_index(np.argmax(patch), patch.shape)\n",
        "                    output[i, j, k] = patch[max_idx]\n",
        "\n",
        "                    # Store indices for backpropagation\n",
        "                    pool_indices[i, j, k] = [h_start + max_idx[0], w_start + max_idx[1]]\n",
        "                    #pool_indices[i, j, k, 0] = h_start + max_idx[0]\n",
        "                    #pool_indices[i, j, k, 1] = w_start + max_idx[1]\n",
        "        return output, pool_indices\n",
        "\n",
        "    def softmax(self, z):\n",
        "        \"\"\"\n",
        "        Apply softmax activation.\n",
        "\n",
        "        Args:\n",
        "            z: Input logits\n",
        "\n",
        "        Returns:\n",
        "            Probability distribution\n",
        "        \"\"\"\n",
        "        exp_z = np.exp(z - np.max(z))  # Subtract max for numerical stability\n",
        "        return exp_z / np.sum(exp_z)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Perform forward pass through the entire network.\n",
        "\n",
        "        Args:\n",
        "            x: Input image of shape (H, W) for grayscale\n",
        "\n",
        "        Returns:\n",
        "            Probability distribution over classes\n",
        "        \"\"\"\n",
        "        # Store the input\n",
        "        self.cache['input'] = x\n",
        "        # First convolutional layer\n",
        "        conv1_output = self.conv_forward(x, self.filters1)\n",
        "        self.cache['conv1_output'] = conv1_output\n",
        "        # First max pooling layer\n",
        "        pool1_output, pool1_indices = self.pool_forward(conv1_output)\n",
        "        self.cache['pool1_output'] = pool1_output\n",
        "        self.cache['pool1_indices'] = pool1_indices\n",
        "\n",
        "        # Second convolutional layer\n",
        "        conv2_output = self.conv_forward(pool1_output, self.filters2)\n",
        "        self.cache['conv2_output'] = conv2_output\n",
        "\n",
        "        # Second max pooling layer\n",
        "        pool2_output, pool2_indices = self.pool_forward(conv2_output)\n",
        "        self.cache['pool2_output'] = pool2_output\n",
        "        self.cache['pool2_indices'] = pool2_indices\n",
        "        # Flatten output\n",
        "        flattened = pool2_output.reshape(1, -1)\n",
        "        self.cache['flattened'] = flattened\n",
        "\n",
        "        # Fully connected layer\n",
        "        fc_output = np.dot(self.W, flattened.T) + self.b\n",
        "        self.cache['fc_output'] = fc_output\n",
        "\n",
        "        # Softmax\n",
        "        probs = self.softmax(fc_output.flatten())\n",
        "        self.cache['probs'] = probs\n",
        "\n",
        "        return probs\n",
        "\n",
        "    def conv_backward(self, dout, cache, filters):\n",
        "        \"\"\"\n",
        "        Backward pass for convolutional layer.\n",
        "\n",
        "        Args:\n",
        "            dout: Gradient from output\n",
        "            cache: Cached values from forward pass\n",
        "            filters: Filters used in forward pass\n",
        "\n",
        "        Returns:\n",
        "            Gradients for filters\n",
        "        \"\"\"\n",
        "        x = cache['input']\n",
        "        if len(x.shape) == 2:\n",
        "            x = x.reshape(x.shape[0], x.shape[1], 1)\n",
        "\n",
        "        h, w, c = x.shape\n",
        "        num_filters, kh, kw, _ = filters.shape\n",
        "        dfilters = np.zeros_like(filters)\n",
        "\n",
        "        # Calculate dfilters\n",
        "        for f in range(num_filters):\n",
        "            for i in range(dout.shape[0]):\n",
        "                for j in range(dout.shape[1]):\n",
        "                    # Extract patch from input\n",
        "                    patch = x[i:i+kh, j:j+kw, :]\n",
        "                    # Update filter gradients\n",
        "                    dfilters[f] += patch * dout[i, j, f]\n",
        "\n",
        "        return dfilters\n",
        "\n",
        "    def pool_backward(self, dout, cache, pool_indices):\n",
        "        \"\"\"\n",
        "        Backward pass for max pooling layer.\n",
        "\n",
        "        Args:\n",
        "            dout: Gradient from output\n",
        "            cache: Cached values from forward pass\n",
        "            pool_indices: Indices of max values from forward pass\n",
        "\n",
        "        Returns:\n",
        "            Gradient for input to pooling layer\n",
        "        \"\"\"\n",
        "        dpool = np.zeros_like(cache)\n",
        "\n",
        "        # Unpool (distribute gradients to the max locations)\n",
        "        for i in range(dout.shape[0]):\n",
        "            for j in range(dout.shape[1]):\n",
        "                for k in range(dout.shape[2]):\n",
        "                    h_idx, w_idx = pool_indices[i, j, k]\n",
        "                    dpool[h_idx, w_idx, k] = dout[i, j, k]\n",
        "\n",
        "        return dpool\n",
        "    def save_weights(self, filepath):\n",
        "        \"\"\"\n",
        "        Save model weights to a file.\n",
        "\n",
        "        Args:\n",
        "            filepath: Path to save the weights (should end with .pkl or .npz)\n",
        "        \"\"\"\n",
        "        weights = {\n",
        "            'filters1': self.filters1,\n",
        "            'filters2': self.filters2,\n",
        "            'W': self.W,\n",
        "            'b': self.b,\n",
        "            'input_shape': self.input_shape,\n",
        "            'num_classes': self.num_classes\n",
        "        }\n",
        "\n",
        "        if filepath.endswith('.pkl'):\n",
        "            with open(filepath, 'wb') as f:\n",
        "                pickle.dump(weights, f)\n",
        "        elif filepath.endswith('.npz'):\n",
        "            np.savez(filepath, **weights)\n",
        "        else:\n",
        "            raise ValueError(\"Filepath should end with .pkl or .npz\")\n",
        "\n",
        "        print(f\"Weights saved successfully to {filepath}\")\n",
        "\n",
        "    def load_weights(self, filepath):\n",
        "        \"\"\"\n",
        "        Load model weights from a file.\n",
        "\n",
        "        Args:\n",
        "            filepath: Path to load the weights from\n",
        "        \"\"\"\n",
        "        if not os.path.exists(filepath):\n",
        "            raise FileNotFoundError(f\"Weights file not found at {filepath}\")\n",
        "\n",
        "        if filepath.endswith('.pkl'):\n",
        "            with open(filepath, 'rb') as f:\n",
        "                weights = pickle.load(f)\n",
        "        elif filepath.endswith('.npz'):\n",
        "            weights = np.load(filepath)\n",
        "            weights = {key: weights[key] for key in weights.files}\n",
        "        else:\n",
        "            raise ValueError(\"Filepath should end with .pkl or .npz\")\n",
        "\n",
        "        # Verify architecture matches\n",
        "        if tuple(weights['input_shape']) != self.input_shape:\n",
        "            raise ValueError(\"Input shape doesn't match model architecture\")\n",
        "        if weights['num_classes'] != self.num_classes:\n",
        "            raise ValueError(\"Number of classes doesn't match model architecture\")\n",
        "\n",
        "        # Load weights\n",
        "        self.filters1 = weights['filters1']\n",
        "        self.filters2 = weights['filters2']\n",
        "        self.W = weights['W']\n",
        "        self.b = weights['b']\n",
        "\n",
        "        print(f\"Weights loaded successfully from {filepath}\")\n",
        "\n",
        "    def get_weights_dict(self):\n",
        "        \"\"\"\n",
        "        Return a dictionary of all trainable weights.\n",
        "\n",
        "        Returns:\n",
        "            Dictionary containing all weights and biases\n",
        "        \"\"\"\n",
        "        return {\n",
        "            'filters1': self.filters1,\n",
        "            'filters2': self.filters2,\n",
        "            'W': self.W,\n",
        "            'b': self.b\n",
        "        }\n",
        "\n",
        "    def set_weights(self, weights_dict):\n",
        "        \"\"\"\n",
        "        Set weights from a dictionary.\n",
        "\n",
        "        Args:\n",
        "            weights_dict: Dictionary containing weights to load\n",
        "        \"\"\"\n",
        "        self.filters1 = weights_dict.get('filters1', self.filters1)\n",
        "        self.filters2 = weights_dict.get('filters2', self.filters2)\n",
        "        self.W = weights_dict.get('W', self.W)\n",
        "        self.b = weights_dict.get('b', self.b)\n",
        "    def backward(self, y_true):\n",
        "        \"\"\"\n",
        "        Perform backpropagation and update weights.\n",
        "\n",
        "        Args:\n",
        "            y_true: True class index\n",
        "        \"\"\"\n",
        "        # Initialize gradients\n",
        "        dW = np.zeros_like(self.W)\n",
        "        db = np.zeros_like(self.b)\n",
        "        dfilters1 = np.zeros_like(self.filters1)\n",
        "        dfilters2 = np.zeros_like(self.filters2)\n",
        "\n",
        "        # Retrieve cached values\n",
        "        probs = self.cache['probs']\n",
        "        fc_output = self.cache['fc_output']\n",
        "        flattened = self.cache['flattened']\n",
        "        pool2_output = self.cache['pool2_output']\n",
        "        pool2_indices = self.cache['pool2_indices']\n",
        "        conv2_output = self.cache['conv2_output']\n",
        "        pool1_output = self.cache['pool1_output']\n",
        "        pool1_indices = self.cache['pool1_indices']\n",
        "        conv1_output = self.cache['conv1_output']\n",
        "        input_img = self.cache['input']\n",
        "\n",
        "        # Softmax gradient - Cross-entropy loss\n",
        "        dout = probs.copy()\n",
        "        dout[y_true] -= 1\n",
        "\n",
        "        # Gradient for fully connected layer\n",
        "        dfc = dout.reshape(-1, 1)\n",
        "        dW = np.dot(dfc, flattened)\n",
        "        db = dfc\n",
        "\n",
        "        # Gradient for flattened output\n",
        "        dflattened = np.dot(self.W.T, dfc)\n",
        "        dpool2 = dflattened.reshape(pool2_output.shape)\n",
        "\n",
        "        # Gradient through second max pooling layer\n",
        "        dconv2 = self.pool_backward(dpool2, conv2_output, pool2_indices)\n",
        "\n",
        "        # Gradient through ReLU in second conv layer\n",
        "        dconv2_relu = dconv2 * (conv2_output > 0)\n",
        "\n",
        "        # Gradient for second conv filters\n",
        "        self.cache['input'] = pool1_output  # Set input for conv_backward\n",
        "        dfilters2 = self.conv_backward(dconv2_relu, self.cache, self.filters2)\n",
        "\n",
        "        # Gradient to first pooling output\n",
        "        dpool1 = np.zeros_like(pool1_output)\n",
        "        for f in range(self.filters2.shape[0]):\n",
        "            for i in range(dconv2_relu.shape[0]):\n",
        "                for j in range(dconv2_relu.shape[1]):\n",
        "                    patch = pool1_output[i:i+3, j:j+3, :]\n",
        "                    for c in range(patch.shape[2]):\n",
        "                        dpool1[i:i+3, j:j+3, c] += self.filters2[f, :, :, c] * dconv2_relu[i, j, f]\n",
        "\n",
        "        # Gradient through first max pooling layer\n",
        "        dconv1 = self.pool_backward(dpool1, conv1_output, pool1_indices)\n",
        "\n",
        "        # Gradient through ReLU in first conv layer\n",
        "        dconv1_relu = dconv1 * (conv1_output > 0)\n",
        "\n",
        "        # Gradient for first conv filters\n",
        "        self.cache['input'] = input_img  # Set input for conv_backward\n",
        "        dfilters1 = self.conv_backward(dconv1_relu, self.cache, self.filters1)\n",
        "\n",
        "        # Update weights and biases\n",
        "        self.W -= self.lr * dW\n",
        "        self.b -= self.lr * db\n",
        "        self.filters1 -= self.lr * dfilters1\n",
        "        self.filters2 -= self.lr * dfilters2\n",
        "\n",
        "    def evaluate(self, X, y):\n",
        "        \"\"\"\n",
        "        Evaluate the model on the given data.\n",
        "\n",
        "        Args:\n",
        "            X: Input data\n",
        "            y: Ground truth labels\n",
        "\n",
        "        Returns:\n",
        "            Tuple of (predictions, loss, accuracy)\n",
        "        \"\"\"\n",
        "        loss = 0\n",
        "        correct = 0\n",
        "        predictions = []\n",
        "\n",
        "        for i in range(len(X)):\n",
        "            probs = self.forward(X[i])\n",
        "            predictions.append(np.argmax(probs))  # Store the predicted class\n",
        "            loss += -np.log(probs[y[i]] + 1e-10)\n",
        "            correct += int(np.argmax(probs) == y[i])\n",
        "\n",
        "        return np.array(predictions), loss / len(X), correct / len(X)\n",
        "\n",
        "    def visualize_filters(self, layer=1, figsize=(10, 5)):\n",
        "        \"\"\"\n",
        "        Visualize the learned convolutional filters.\n",
        "\n",
        "        Args:\n",
        "            layer: Which convolutional layer to visualize (1 or 2)\n",
        "            figsize: Size of the figure\n",
        "        \"\"\"\n",
        "        if layer == 1:\n",
        "            filters = self.filters1\n",
        "            title = \"First Convolutional Layer Filters\"\n",
        "        elif layer == 2:\n",
        "            filters = self.filters2\n",
        "            title = \"Second Convolutional Layer Filters\"\n",
        "        else:\n",
        "            raise ValueError(\"Layer must be 1 or 2\")\n",
        "\n",
        "        num_filters = filters.shape[0]\n",
        "        filter_size = filters.shape[1]\n",
        "        input_channels = filters.shape[3]\n",
        "\n",
        "        # For first layer with grayscale input, we can show all filters in one row\n",
        "        if layer == 1:\n",
        "            plt.figure(figsize=figsize)\n",
        "            for i in range(num_filters):\n",
        "                plt.subplot(1, num_filters, i+1)\n",
        "                plt.imshow(filters[i, :, :, 0], cmap='gray')\n",
        "                plt.axis('off')\n",
        "                plt.title(f'Filter {i+1}')\n",
        "            plt.suptitle(title)\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "        # For second layer with multiple input channels, we show each filter's channels separately\n",
        "        elif layer == 2:\n",
        "            # Calculate grid size\n",
        "            rows = num_filters\n",
        "            cols = input_channels\n",
        "            fig, axes = plt.subplots(rows, cols, figsize=(cols*2, rows*1.5))\n",
        "\n",
        "            if num_filters == 1:\n",
        "                axes = [axes]  # Handle case with single filter\n",
        "\n",
        "            for i in range(num_filters):\n",
        "                for j in range(input_channels):\n",
        "                    if num_filters > 1:\n",
        "                        ax = axes[i, j]\n",
        "                    else:\n",
        "                        ax = axes[j]\n",
        "\n",
        "                    ax.imshow(filters[i, :, :, j], cmap='gray')\n",
        "                    ax.axis('off')\n",
        "\n",
        "                    # Only show filter number on first column\n",
        "                    if j == 0:\n",
        "                        ax.set_ylabel(f'Filter {i+1}', rotation=0, ha='right', va='center')\n",
        "\n",
        "                    # Only show channel number on first row\n",
        "                    if i == 0:\n",
        "                        ax.set_title(f'Ch {j+1}')\n",
        "\n",
        "            plt.suptitle(f\"{title} (Each row shows one filter's channels)\")\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "    def visualize_all_filters(self, figsize=(15, 8)):\n",
        "        \"\"\"\n",
        "        Visualize filters from both convolutional layers.\n",
        "        \"\"\"\n",
        "        plt.figure(figsize=figsize)\n",
        "\n",
        "        # First layer filters\n",
        "        plt.subplot(1, 2, 1)\n",
        "        for i in range(self.filters1.shape[0]):\n",
        "            plt.subplot(1, self.filters1.shape[0], i+1)\n",
        "            plt.imshow(self.filters1[i, :, :, 0], cmap='gray')\n",
        "            plt.axis('off')\n",
        "            plt.title(f'L1 Filter {i+1}')\n",
        "        plt.suptitle('First Layer Filters')\n",
        "\n",
        "        # Second layer filters (show first filter's channels as example)\n",
        "        plt.figure(figsize=figsize)\n",
        "        for j in range(self.filters2.shape[3]):\n",
        "            plt.subplot(1, self.filters2.shape[3], j+1)\n",
        "            plt.imshow(self.filters2[0, :, :, j], cmap='gray')\n",
        "            plt.axis('off')\n",
        "            plt.title(f'L2 Filter 1 Ch{j+1}')\n",
        "        plt.suptitle('Second Layer Filter 1 Channels')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def train(self, X, y, X_test, y_test, epochs=5, batch_size=32):\n",
        "        \"\"\"\n",
        "        Train the CNN model.\n",
        "\n",
        "        Args:\n",
        "            X: Training data\n",
        "            y: Training labels\n",
        "            X_test: Test data\n",
        "            y_test: Test labels\n",
        "            epochs: Number of epochs\n",
        "            batch_size: Batch size\n",
        "        \"\"\"\n",
        "        for epoch in range(epochs):\n",
        "            indices = np.arange(len(X))\n",
        "            np.random.shuffle(indices)\n",
        "            X, y = X[indices], y[indices]\n",
        "            loss = 0\n",
        "            acc = 0\n",
        "\n",
        "            for start in range(0, len(X), batch_size):\n",
        "                end = min(start + batch_size, len(X))\n",
        "                batch_X, batch_y = X[start:end], y[start:end]\n",
        "\n",
        "                batch_loss = 0\n",
        "                batch_acc = 0\n",
        "\n",
        "                for i in range(len(batch_X)):\n",
        "                    x = batch_X[i]\n",
        "                    y_true = batch_y[i]\n",
        "\n",
        "                    # Forward pass\n",
        "                    probs = self.forward(x)\n",
        "                    #print(\"Forward pass done\")\n",
        "\n",
        "                    # Compute loss\n",
        "                    batch_loss += -np.log(probs[y_true] + 1e-10)\n",
        "                    batch_acc += int(np.argmax(probs) == y_true)\n",
        "                    #print(\"Loss and accuracy computed\")\n",
        "\n",
        "                    # Backward pass\n",
        "                    self.backward(y_true)\n",
        "                    #print(\"Backward pass done\")\n",
        "\n",
        "                loss += batch_loss\n",
        "                acc += batch_acc\n",
        "\n",
        "                # Print progress\n",
        "                if (start // batch_size) % 1 == 0:\n",
        "                      print(f\"Epoch {epoch+1}, Batch {start//batch_size + 1}/{len(X)//batch_size + 1}, \" +\n",
        "                      f\"Loss: {float(batch_loss)/len(batch_X):.4f}, Acc: {float(batch_acc)/len(batch_X):.4f}\")\n",
        "\n",
        "            # Evaluate on test set\n",
        "            _, _, epoch_test_acc = self.evaluate(X_test, y_test)\n",
        "            print(f\"Epoch {epoch+1} complete - Loss: {float(loss)/len(X):.4f}, Train Acc: {float(acc)/len(X):.4f}, Test Acc: {epoch_test_acc:.4f}\")\n",
        "\n",
        "    def plot_confusion_matrix(self, y_true, y_pred, class_names):\n",
        "        cm = confusion_matrix(y_true, y_pred)\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                    xticklabels=class_names, yticklabels=class_names)\n",
        "        plt.xlabel(\"Predicted\")\n",
        "        plt.ylabel(\"True\")\n",
        "        plt.title(\"Confusion Matrix\")\n",
        "        plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l8ppfbbm51Rm",
        "outputId": "f78544c5-d34f-43eb-842e-d43380a3b55f"
      },
      "outputs": [],
      "source": [
        "epochs = 5\n",
        "learning_rate = 0.01\n",
        "batch_size = 32\n",
        "cnn = DeepCNN(input_shape=(100, 100), num_classes=len(class_names), learning_rate=learning_rate)\n",
        "cnn.train(X_train, y_train, X_test, y_test, epochs=epochs, batch_size=batch_size)\n",
        "# Visualize filters from the first convolutional layer\n",
        "cnn.visualize_filters(layer=1, figsize=(10, 5))\n",
        "\n",
        "# Visualize filters from the second convolutional layer\n",
        "cnn.visualize_filters(layer=2, figsize=(10, 5))\n",
        "weights_filepath = rf\"C:\\Users\\kingm\\Downloads\\CNN_weights\\CNN_weights_epochs{epochs}_lr{learning_rate}_bs{batch_size}.npz\"\n",
        "\n",
        "cnn.save_weights(weights_filepath)\n",
        "preds ,loss , test_acc = cnn.evaluate(X_test, y_test)\n",
        "\n",
        "cnn.plot_confusion_matrix(y_test, preds, class_names)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
